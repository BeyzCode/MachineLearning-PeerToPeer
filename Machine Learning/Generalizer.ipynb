{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133839,
     "status": "ok",
     "timestamp": 1735809463542,
     "user": {
      "displayName": "Fabiano Abbey Karo Sekali",
      "userId": "16529881993509846570"
     },
     "user_tz": -420
    },
    "id": "-3ql5HxfnnDl",
    "outputId": "24de0b76-d249-4301-94bf-5fec76aa4d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC1 predictions (train): [0 0 1 0 1 0 1 1 1 1]\n",
      "SVC2 predictions (train): [0 0 1 0 1 0 1 1 1 1]\n",
      "SVC3 predictions (train): [0 0 1 0 1 1 1 1 1 1]\n",
      "Meta features (train): [[0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "SVC1 predictions (test): [1 0 0 1 0 1 1 1 0 0]\n",
      "SVC2 predictions (test): [1 0 0 1 0 1 1 1 0 0]\n",
      "SVC3 predictions (test): [1 0 0 1 0 1 1 1 0 0]\n",
      "Meta features (test): [[1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "Classification Report (Meta-Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BENIGN       1.00      1.00      1.00     39152\n",
      "        DDoS       1.00      1.00      1.00     51146\n",
      "\n",
      "    accuracy                           1.00     90298\n",
      "   macro avg       1.00      1.00      1.00     90298\n",
      "weighted avg       1.00      1.00      1.00     90298\n",
      "\n",
      "F1 Score (Meta-Model): 0.9975192430261999\n",
      "Accuracy Score (Meta-Model): 0.9975193249019911\n",
      "Meta-model has been saved as 'meta_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "# Load model yang sudah dilatih sebelumnya (SVC1, SVC2, SVC3)\n",
    "svc1 = joblib.load(\"svc1_node1_model.pkl\")  \n",
    "svc2 = joblib.load(\"svc2_Node2_model.pkl\")  \n",
    "svc3 = joblib.load(\"svc3_Node3_model.pkl\")  \n",
    "\n",
    "# Membuat meta-model (Generalizer) dengan Logistic Regression\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Membuat ensemble stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('svc1', svc1),\n",
    "        ('svc2', svc2),\n",
    "        ('svc3', svc3)\n",
    "    ],\n",
    "    final_estimator=meta_model\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "data.copy()\n",
    "data_filled = data.fillna(0)\n",
    "\n",
    "# Ekstraksi fitur dan label\n",
    "X = data_filled[[' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
    "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "       ' Total Length of Bwd Packets', 'Flow Bytes/s', ' Flow Packets/s']].values\n",
    "y = data_filled[' Label']\n",
    "\n",
    "# Ganti nilai infinity dengan 0\n",
    "X = np.where(np.isinf(X), 0, X)\n",
    "\n",
    "# Split data untuk training dan testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Imputasi nilai kosong\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Standardisasi data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Generate meta-features (prediksi dari svc1, svc2, svc3)\n",
    "try:\n",
    "\n",
    "    svc1_train_predictions = svc1.predict(X_train)\n",
    "    svc2_train_predictions = svc2.predict(X_train)\n",
    "    svc3_train_predictions = svc3.predict(X_train)\n",
    "\n",
    "    print(\"SVC1 predictions (train):\", svc1_train_predictions[:10])  \n",
    "    print(\"SVC2 predictions (train):\", svc2_train_predictions[:10])  \n",
    "    print(\"SVC3 predictions (train):\", svc3_train_predictions[:10])  \n",
    "\n",
    "    # Prediksi dari nodes (Data Test)\n",
    "    meta_features_train = np.column_stack([\n",
    "        svc1_train_predictions,  \n",
    "        svc2_train_predictions,  \n",
    "        svc3_train_predictions,  \n",
    "    ])\n",
    "    print(\"Meta features (train):\", meta_features_train[:10])  \n",
    "\n",
    "    svc1_test_predictions = svc1.predict(X_test)\n",
    "    svc2_test_predictions = svc2.predict(X_test)\n",
    "    svc3_test_predictions = svc3.predict(X_test)\n",
    "\n",
    "    print(\"SVC1 predictions (test):\", svc1_test_predictions[:10])\n",
    "    print(\"SVC2 predictions (test):\", svc2_test_predictions[:10])\n",
    "    print(\"SVC3 predictions (test):\", svc3_test_predictions[:10])\n",
    "\n",
    "    # Prediksi dari nodes (Data Uji)\n",
    "    meta_features_test = np.column_stack([\n",
    "        svc1_test_predictions,   \n",
    "        svc2_test_predictions,   \n",
    "        svc3_test_predictions,  \n",
    "    ])\n",
    "    print(\"Meta features (test):\", meta_features_test[:10])  \n",
    "\n",
    "    # Training meta-model (Logistic Regression)\n",
    "    meta_model.fit(meta_features_train, y_train)\n",
    "\n",
    "    # Evaluasi meta-model\n",
    "    y_pred_meta = meta_model.predict(meta_features_test)\n",
    "\n",
    "    print(\"Classification Report (Meta-Model):\")\n",
    "    print(classification_report(y_test, y_pred_meta))\n",
    "    print(\"F1 Score (Meta-Model):\", f1_score(y_test, y_pred_meta, average='weighted'))\n",
    "    print(\"Accuracy Score (Meta-Model):\", accuracy_score(y_test, y_pred_meta))\n",
    "\n",
    "    # Save meta-model (Generalizer)\n",
    "    joblib.dump(meta_model, \"meta_model.pkl\")\n",
    "    print(\"Meta-model has been saved as 'meta_model.pkl'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error during meta-model training or evaluation:\", str(e))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNyA2JLAexd2QrBh/9eoMOb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
